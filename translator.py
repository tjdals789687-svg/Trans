# translator.py
# ë²ˆì—­ ê¸°ëŠ¥ ë‹´ë‹¹ ëª¨ë“ˆ
# ë‹´ë‹¹: ë²ˆì—­ íŒ€ì›

from transformers import MarianMTModel, MarianTokenizer
import torch
from config import SUPPORTED_LANGUAGE_PAIRS, MAX_TOKEN_LENGTH

# ============================================
# ëª¨ë¸ ìºì‹œ
# ============================================
_model_cache = {}

def get_model(source_lang: str, target_lang: str):
    """
    ë²ˆì—­ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ìºì‹± ì ìš©)
    
    Args:
        source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'en')
        target_lang: ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko')
    
    Returns:
        (tokenizer, model) íŠœí”Œ ë˜ëŠ” (None, None)
    """
    cache_key = f"{source_lang}-{target_lang}"
    
    # ìºì‹œ í™•ì¸
    if cache_key in _model_cache:
        print(f"âœ… [Translator] ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë“œ: {cache_key}")
        return _model_cache[cache_key]
    
    # ìƒˆë¡œ ë¡œë“œ
    print(f"ğŸ“¥ [Translator] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {cache_key}")
    
    try:
        model_name = f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}"
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
        
        # ìºì‹œì— ì €ì¥
        _model_cache[cache_key] = (tokenizer, model)
        print(f"âœ… [Translator] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {cache_key}")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ [Translator] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {cache_key} - {str(e)}")
        return None, None


def translate(text: str, source_lang: str, target_lang: str) -> dict:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
    
    Args:
        text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
        source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ
        target_lang: ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ
    
    Returns:
        {
            'success': bool,
            'translated': str or None,
            'error': str or None
        }
    """
    # ì–¸ì–´ ìŒ ìœ íš¨ì„± ê²€ì‚¬
    lang_pair = f"{source_lang}-{target_lang}"
    if lang_pair not in SUPPORTED_LANGUAGE_PAIRS:
        return {
            'success': False,
            'translated': None,
            'error': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ ìŒ: {lang_pair}'
        }
    
    # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    tokenizer, model = get_model(source_lang, target_lang)
    
    if tokenizer is None or model is None:
        return {
            'success': False,
            'translated': None,
            'error': f'ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lang_pair}'
        }
    
    try:
        # í† í°í™”
        inputs = tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_TOKEN_LENGTH
        )
        
        # ë²ˆì—­ ìˆ˜í–‰
        with torch.no_grad():
            translated = model.generate(**inputs)
        
        # ë””ì½”ë”©
        result = tokenizer.decode(translated[0], skip_special_tokens=True)
        
        return {
            'success': True,
            'translated': result,
            'error': None
        }
        
    except Exception as e:
        return {
            'success': False,
            'translated': None,
            'error': f'ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {str(e)}'
        }


def get_cached_models() -> list:
    """í˜„ì¬ ìºì‹œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return list(_model_cache.keys())


def clear_cache():
    """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ í•´ì œ)"""
    global _model_cache
    _model_cache = {}
    print("ğŸ—‘ï¸ [Translator] ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”ë¨")


# ============================================
# í…ŒìŠ¤íŠ¸ìš©
# ============================================
if __name__ == '__main__':
    # ë‹¨ë… ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    print("=" * 50)
    print("ğŸ§ª Translator ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    test_text = "Hello, how are you?"
    print(f"\nì›ë³¸: {test_text}")
    
    result = translate(test_text, 'en', 'ko')
    
    if result['success']:
        print(f"ë²ˆì—­: {result['translated']}")
    else:
        print(f"ì—ëŸ¬: {result['error']}")
    
    print(f"\nìºì‹œëœ ëª¨ë¸: {get_cached_models()}")
